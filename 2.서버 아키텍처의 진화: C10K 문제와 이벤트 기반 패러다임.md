# 1. C10K 문제란?

- **"서버 한 대가 어떻게 1만 개의 동시 연결을 효율적으로 처리할 것인가?"** 에 대한 기술적 도전 과제
- 단순히 초당 요청 처리(TPS)가 아닌, **연결을 끊지 않고 1만 개의 연결을 동시 유지**하는 것이 핵심
- **주요 사용 예시**: 실시간 채팅, 라이브 스트리밍, 대규모 온라인 게임, IoT

# 2. 왜 등장했을까?

- 1990년대 말, 인터넷 서비스가 폭발적으로 성장하며 동시 접속자 수가 급증
- 당시 서버 아키텍처는 동시 접속자 수가 수백~수천만 넘어도 **성능 저하**, **연결 거부**, **서버 다운** 현상이 빈번하게 발생

# 3. 전통 방식의 문제점: 연결 당 스레드 할당

- 클라이언트 연결 1개당 스레드 1개를 생성하는 방식
- **과도한 메모리 소모**
    - 스레드 1개가 1MB 스택 사용 시, 1만 개 스레드는 **10GB 메모리** 소모
- **높은 CPU 컨텍스트 스위칭 비용**
    - 수많은 스레드를 전환하는 비용이 커져, 실제 작업보다 스레드 관리에 자원을 더 낭비

→ **결과**: 적은 동시 접속자 수에도 서버가 버티지 못함

# 4. 해결책: 이벤트 기반 아키텍처

- **핵심 질문**: "더 적은 스레드로 수많은 I/O를 처리할 방법은?"
- **해답**: **이벤트 기반 아키텍처 (Event-Driven Architecture)**
    - 하나의 스레드(이벤트 루프)가 여러 연결을 감시하다, **이벤트가 발생했을 때만** 작업을 처리하는 방식
    - **핵심 기술**: Non-blocking I/O + I/O 멀티플렉싱
    - **대표 주자**: Nginx, Node.js, Redis

# 5. I/O 멀티플렉싱이란?

- 하나의 스레드가 여러 개의 I/O(소켓) 채널을 동시에 감시하고 관리하는 기술
- 동작 원리
    1. 감시할 소켓들을 하나의 세트로 묶어 커널에 등록
    2. 커널에 “이 소켓들 중 하나라도 데이터가 준비되면 알려줘”라고 요청
    3. 이벤트가 발생한 소켓이 생길 때까지 스레드는 대기(Blocking)
    4. 커널이 이벤트 발생을 알리면, 스레드는 해당 소켓에 대한 I/O 작업만 수행

→ 결과적으로 하나의 스레드로 여러 클라이언트의 요청을 동시에 처리하는 효과를 냄

# 6. 핵심 기술: epoll 동작 방식 (Linux)

- `epoll`은 기존 `select`/`poll`의 단점을 개선한 고성능 I/O 멀티플렉싱 모델
- 주요 구성 요소
    - **epoll 인스턴스**: 커널 내부에 생성되는 이벤트 저장 공간
    - **관심 목록(Interest List)**: 감시할 소켓(fd)들의 목록. 커널 공간에 저장됨
    - **준비 목록(Ready List)**: 이벤트가 발생한 소켓들의 목록
- **동작 예시 (데이터 수신)**
    1. **등록**: `epoll_ctl`을 사용해 감시할 소켓들을 커널의 **관심 목록**에 한 번만 등록
    2. **이벤트 발생**: 클라이언트 데이터가 네트워크 인터페이스(NIC)를 통해 **소켓의 수신 버퍼**에 도착
    3. **커널 작업**: 커널은 데이터가 도착한 소켓을 **준비 목록**에 추가하고, `EPOLLIN`(읽을 데이터 존재) 이벤트를 기록
    4. **통지**: `epoll_wait`는 대기 상태에서 깨어나, 이벤트가 발생한 소켓 목록(**준비 목록**)만 애플리케이션에 즉시 반환

# 7. I/O 멀티플렉싱 기술의 발전

- `select`/`poll`: 커널 내부에서 **폴링(polling)**을 통해 모든 감시 대상을 직접 확인
- `epoll` / `kqueue`: 커널 내부에서 감시 대상에 이벤트 발생 시 커널이 후킹(hooking)하여 알려주는 방식

| 구분 | 구세대: `select` / `poll` |  신세대: `epoll` / `kqueue` |
| --- | --- | --- |
| 동작 방식 | 감시할 모든 연결 목록을 **매번 전체 스캔** | 커널이 **이벤트가 발생한 연결만** 알려줌 |
| 성능 | 연결 수가 많아지면 성능 **급격히 저하** | 연결 수에 상관없이 **일정한 성능 유지** |
| 효율성 | 불필요한 검사로 인한 비효율 발생 | 필요한 작업만 수행하여 매우 효율적 |

# 8. 비동기 서버 디자인 패턴: 리액터 vs 프로액터

- 이벤트 기반 아키텍처는 주로 두 가지 디자인 패턴으로 구현됨
- 핵심 차이는 **"누가 I/O 작업을 직접 수행하는가"**

| 구분 | 리액터 (Reactor) 패턴 | 프로액터 (Proactor) 패턴 |
| --- | --- | --- |
| 핵심 역할 | "I/O 작업이 **준비되었음**을 알려줄게" | "I/O 작업을 **완료했음**을 알려줄게" |
| 동작 방식 | 1. 이벤트 감지기가 I/O 준비 상태를 감지<br>2. 핸들러(애플리케이션)가 직접 I/O 작업 수행 | 1. OS/커널이 비동기 I/O 작업을 직접 수행<br>2. 작업 완료 후 핸들러에 결과 전달 |
| 주요 예시 | **epoll, kqueue**, **Node.js, Netty** | **IOCP** (Windows) |

# 9. 서버 I/O 모델 비교

| 구분 | 과거: Thread-per-Connection | **현재: Event-Driven** |
| --- | --- | --- |
| I/O 방식 | Blocking I/O | Non-blocking I/O |
| 구조 | 연결마다 스레드 생성 | 단일/소수 스레드가 모든 연결 처리 |
| 자원 사용 | 메모리, CPU 낭비 심함 | 최소한의 자원으로 최대 효율 |
| 동시성 | 수백 ~ 수천 개 처리 한계 | 수만 ~ 수십만 개 처리 가능 |

# 10. JVM 서버 모델: Tomcat (BIO)

- 전통적인 **Thread-per-Connection** 모델
- **동작 흐름**
    1. **Acceptor 스레드**가 클라이언트의 연결 요청을 대기
    2. 연결 요청이 오면, Acceptor는 새로운 연결(소켓)을 생성
    3. 생성된 연결을 **스레드 풀(Thread Pool)**의 **Worker 스레드** 중 하나에 할당
    4. Worker 스레드는 해당 연결의 모든 I/O 작업을 **Blocking 방식**으로 처리
- **문제점**
    - **Idle 스레드 문제**:
        - 대부분의 Worker 스레드가 I/O를 기다리며 **아무 일도 하지 않는(Idle) 상태**로 대기.
        - 스레드 풀이 한계에 도달한 경우, 기존 연결자가 아무일도 하지 않아도 새로운 연결을 할당할 수 없음.
    - **심각한 컨텍스트 스위칭**: 연결 수가 많아질수록 스레드 전환 비용이 급증하여 성능 저하.
    - **구조적 한계**: 1연결-1스레드 구조는 메모리 소모와 성능 오버헤드가 극심하여, 구조적으로 많은 동시 연결(C10K)을 유지하는 것이 불가능.

# 11. JVM 서버 모델: Tomcat (NIO)

- Java NIO의 **Selector**를 사용하는 멀티플렉싱 모델 (Linux에서는 **epoll**로 동작)
- **동작 흐름**
    1. **Acceptor 스레드**가 연결을 수락하고, 해당 소켓을 **Poller 스레드**에 등록
    2. **Poller 스레드**는 Selector를 통해 여러 소켓의 이벤트를 동시에 감시
    3. 읽을 데이터가 준비되는 등 이벤트가 발생하면, Poller는 **스레드 풀**에서 **Worker 스레드**를 가져와 작업 할당
    4. Worker 스레드가 실제 요청을 처리
- **장점**
    - Poller 스레드가 여러 연결을 관리하므로, **유휴(Idle) 상태의 스레드가 거의 없음**.
    - BIO 모델에 비해 훨씬 적은 스레드로 높은 동시성을 처리
- 단점
    - **컨텍스트 스위칭 오버헤드**: BIO에 비해 적지만, 작업 전달 과정에서 컨텍스 스위치 비용이 존재는 함
    - **스레드 풀 병목**: 유휴 워커스레드가 없고, 작업중인 워커 스레드들이 모두 바쁘다면 병목이 발생함

# 12. JVM 서버 모델: Netty (NIO)

- Tomcat NIO 모델을 더욱 발전시킨 고성능 이벤트 기반 프레임워크
- **동작 흐름**
    1. **Boss 그룹 (스레드)**: 클라이언트의 연결 요청 수락(accept)만 전담
    2. 수락된 연결을 **Worker 그룹 (스레드)** 중 하나에 등록
    3. **Worker 그룹**의 각 스레드는 자신만의 **이벤트 루프(Event Loop)**를 가짐
    4. 이벤트 루프는 자신에게 할당된 여러 연결의 모든 이벤트(읽기, 쓰기 등)를 직접 처리
- 장점
    - **컨텍스트 스위칭 최소화**: 이벤트를 감지한 스레드가 곧바로 작업을 처리하므로, "전달" 과정에서 발생하는 오버헤드가 없음
    - **병목 지점 분산**: 각 이벤트 루프는 자신에게 할당된 연결들만 책임져, 하나의 연결에서 오래 걸리는 작업이 발생해도 다른 이벤트 루프에서는 영향을 주지 않음.
    - **단순화된 동시성 모델**: 하나의 연결에 대한 모든 처리는 항상 같은 스레드에서 순서대로 일어나므로, 동기화 문제를 크게 신경 쓰지 않아도 됨.

# 13. 부하 테스트란?

- **C10K 문제**는 결국 **"우리 서버가 대규모 트래픽을 감당할 수 있는가?"** 라는 질문으로 귀결
- **부하 테스트**는 이 질문에 답하기 위해, 시스템에 의도적으로 부하(Load)를 가하여 **성능 한계점과 병목 지점을 측정**하는 과정
- 주요 목적
    - **성능 검증**: 목표한 동시 접속자 수(예: 1만 명)를 안정적으로 처리하는지 확인
    - **병목 현상 식별**: CPU, 메모리, 네트워크뿐만 아니라 OS 레벨의 숨겨진 한계점을 발견
    - **안정성 확보**: 실제 서비스 투입 전, 잠재적인 문제를 미리 찾아 해결

# 14. 부하 테스트 필수 시스템 설정 (Linux)

- **파일 디스크립터 한계 늘리기**
    - **이유**: 소켓도 파일의 일종. 기본값(1024)이 매우 낮아 1만 개 연결을 만들 수 없음
    
    ```bash
    # 시스템 전체에서 열 수 있는 FD 수
    sysctl -w fs.file-max=110000
    # 단일 프로세스가 최대로 열 수 있는 FD 수
    sysctl -w fs.nr_open=110000
    # 현재 셸 세션이 열 수 있는 FD 수
    ulimit -n 110000
    ```
    
- **소켓 연결 대기 큐(Backlog) 크기 늘리기**
    - **이유**: 순간적인 연결 요청 폭주 시 요청이 무시(drop)되는 것을 방지
    
    ```bash
    # 연결이 완전히 수락되기 전에 대기하는 큐 제한
    sysctl -w net.core.somaxconn=10000
    # SYN 패킷만 받은 상태에서 대기하는 큐 제한(half-open)
    sysctl -w net.ipv4.tcp_max_syn_backlog=10000
    ```
    
- TCP 메모리 버퍼 늘리기
    - **이유**: 다수 연결의 데이터 통신 시 버퍼 부족으로 인한 성능 저하 방지
    
    ```bash
    # TCP 메모리 버퍼 설정
    sysctl -w net.ipv4.tcp_mem="100000000 100000000 100000000"
    ```
    

# 15. 리틀의 법칙**(Little’s Law)

- 모든 안정적인 시스템에 적용되는 성능 공학의 기본 원리
- **`Concurrency = Throughput × Latency`**
    - **Concurrency (동시성)**: 현재 처리 중인 요청 수
    - **Throughput (처리량, TPS)**: 초당 처리 요청 수
    - **Latency (지연 시간)**: 요청 처리 평균 시간
- **핵심 인사이트**: Latency(지연 시간)와 Throughput(처리량)은 **반비례 관계**
    - 외부 API 호출 등으로 Latency가 10배 늘어나면, 서버의 TPS는 10배 낮아진다고 예측 가능

# 16. 리틀의 법칙 활용하기

- 특정 결과가 아래와 같다고 해보자
    
    ```bash
    Running 10s test @ http://127.0.0.1:8080/
      8 threads and 10000 connections
      Thread Stats   Avg      Stdev     Max   +/- Stdev
        Latency     5.71ms    2.31ms   45.5ms   88.10%
        Req/Sec     1.75k   450.85     3.10k    70.00%
      140000 requests in 10.05s, 20.50MB read
    Requests/sec:  13930.34
    Transfer/sec:    2.04MB
    ```
    
    - Latency Avg (평균 지연 시간): 5.71ms
    - Requests/sec (TPS/RPS): 13930.34
- 생각 해보기
    - 이 테스트는 외부 요청이 없어 평균 지연시간이 5.71ms로 매우 낮게 측정됨. 하지만 실제 환경에서 외부 DB 조회나 API 호출로 평균 지연시간이 **50ms**로 늘어난다고 가정하면 어떻게 될까?
    - 리틀의 법칙에 따라, TPS는 약 **1/9 수준으로 하락**할 것으로 예측할 수 있다. 이처럼 시스템의 작은 지연 시간 증가가 전체 처리량에 얼마나 큰 영향을 미치는지 예측하고 대비하는 데 리틀의 법칙을 활용할 수 있다.

# 17. 결론: C10K를 넘어 C10M으로

- C10K는 더 이상 난제가 아닌, 현대 서버의 **기본 성능 기준점**
- 이벤트 기반 아키텍처는 이제 **표준 패러다임**
- 오늘날 서비스는 **C100K(10만), C1M(100만)** 동시 연결을 요구
- C10K 원리를 이해하고, 시스템을 설계하며, 부하 테스트로 검증하는 능력은 **필수 역량**